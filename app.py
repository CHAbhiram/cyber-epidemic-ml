from flask import Flask, request, jsonify, render_template
import numpy as np
import joblib
import pickle
from tensorflow.keras.models import load_model
from scipy.sparse import hstack
import cv2
import re
from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup


app = Flask(__name__)

# Load models
xss_model = load_model('models/xss_model.h5')
url_model = joblib.load('models/BestModel_ExtraTreesClassifier.joblib')
sql_injection_model = joblib.load('models/saved_model.pkl')

# Load additional resources
with open('models/train_bow', 'rb') as f:
    train_bow = pickle.load(f)
import re
import string
import logging

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from urllib.parse import urlparse
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import SGDClassifier, LogisticRegression, RidgeClassifier, Perceptron
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

from tldextract import extract as tld_extract
from tld import get_tld, is_tld
from tld.exceptions import TldDomainNotFound, TldBadUrl, TldIOError

from colorama import Fore
from datetime import datetime
from plotly.subplots import make_subplots
from plotly import graph_objects as go
from wordcloud import WordCloud
from gensim.models import Word2Vec
import tldextract
import hashlib
import whois
import warnings

warnings.filterwarnings("ignore")
from urllib.parse import urlparse

def get_numerical_values(url):
    url = url.replace('www.', '')
    url_len = get_url_length(url)
    letters_count = count_letters(url)
    digits_count = count_digits(url)
    special_chars_count = count_special_chars(url)
    shortened = has_shortening_service(url)
    abnormal = abnormal_url(url)
    secure_https = secure_http(url)
    have_ip = have_ip_address(url)

    parsed_url = urlparse(url)
    netloc_parts = parsed_url.netloc.split(".")

    # Defensive domain extraction
    if len(netloc_parts) >= 2:
        root_domain = netloc_parts[-2]
    else:
        root_domain = parsed_url.netloc  # fallback to entire netloc or empty string

    url_region = get_url_region(root_domain)

    return {
        'url_len': url_len,
        'letters_count': letters_count,
        'digits_count': digits_count,
        'special_chars_count': special_chars_count,
        'shortened': shortened,
        'abnormal': abnormal,
        'secure_http': secure_https,
        'have_ip': have_ip,
        'url_region': hash_encode(url_region),
        'root_domain': hash_encode(root_domain)
    }


def get_url_length(url):
    return len(url)
def extract_pri_domain(url):
    try:
        res = get_tld(url, as_object = True, fail_silently=False,fix_protocol=True)
        pri_domain= res.parsed_url.netloc
    except :
        pri_domain= None
    return pri_domain
def count_letters(url):
    num_letters = sum(char.isalpha() for char in url)
    return num_letters

def count_digits(url):
    num_digits = sum(char.isdigit() for char in url)
    return num_digits
def count_special_chars(url):
    special_chars = "!@#$%^&*()_+-=[]{};:,.<>/?~|"
    num_special_chars = sum(char in special_chars for char in url)
    return num_special_chars
def has_shortening_service(url):
    pattern = re.compile(r'bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                         r'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                         r'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                         r'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                         r'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                         r'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                         r'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                         r'tr\.im|link\.zip\.net')
    match = pattern.search(url)
    return int(bool(match))
def abnormal_url(url):
    parsed_url = urlparse(url)
    hostname = parsed_url.hostname
    if hostname:
        hostname = str(hostname)
        match = re.search(hostname, url)
        if match:
            return 1
    return 0
def secure_http(url):
    scheme = urlparse(url).scheme
    if scheme == 'https':
        return 1
    else:
        return 0
def have_ip_address(url):
    pattern = r'(([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.' \
              r'([01]?\d\d?|2[0-4]\d|25[0-5])\/)|' \
              r'(([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.' \
              r'([01]?\d\d?|2[0-4]\d|25[0-5])\/)|' \
              r'((0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\/)' \
              r'(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|' \
              r'([0-9]+(?:\.[0-9]+){3}:[0-9]+)|' \
              r'((?:(?:\d|[01]?\d\d|2[0-4]\d|25[0-5])\.){3}(?:25[0-5]|2[0-4]\d|[01]?\d\d|\d)(?:\/\d{1,2})?)'

    match = re.search(pattern, url)
    if match:
        return 1
    else:
        return 0
def get_url_region(primary_domain):
    ccTLD_to_region = {
    ".ac": "Ascension Island",
    ".ad": "Andorra",
    ".ae": "United Arab Emirates",
    ".af": "Afghanistan",
    ".ag": "Antigua and Barbuda",
    ".ai": "Anguilla",
    ".al": "Albania",
    ".am": "Armenia",
    ".an": "Netherlands Antilles",
    ".ao": "Angola",
    ".aq": "Antarctica",
    ".ar": "Argentina",
    ".as": "American Samoa",
    ".at": "Austria",
    ".au": "Australia",
    ".aw": "Aruba",
    ".ax": "Åland Islands",
    ".az": "Azerbaijan",
    ".ba": "Bosnia and Herzegovina",
    ".bb": "Barbados",
    ".bd": "Bangladesh",
    ".be": "Belgium",
    ".bf": "Burkina Faso",
    ".bg": "Bulgaria",
    ".bh": "Bahrain",
    ".bi": "Burundi",
    ".bj": "Benin",
    ".bm": "Bermuda",
    ".bn": "Brunei Darussalam",
    ".bo": "Bolivia",
    ".br": "Brazil",
    ".bs": "Bahamas",
    ".bt": "Bhutan",
    ".bv": "Bouvet Island",
    ".bw": "Botswana",
    ".by": "Belarus",
    ".bz": "Belize",
    ".ca": "Canada",
    ".cc": "Cocos Islands",
    ".cd": "Democratic Republic of the Congo",
    ".cf": "Central African Republic",
    ".cg": "Republic of the Congo",
    ".ch": "Switzerland",
    ".ci": "Côte d'Ivoire",
    ".ck": "Cook Islands",
    ".cl": "Chile",
    ".cm": "Cameroon",
    ".cn": "China",
    ".co": "Colombia",
    ".cr": "Costa Rica",
    ".cu": "Cuba",
    ".cv": "Cape Verde",
    ".cw": "Curaçao",
    ".cx": "Christmas Island",
    ".cy": "Cyprus",
    ".cz": "Czech Republic",
    ".de": "Germany",
    ".dj": "Djibouti",
    ".dk": "Denmark",
    ".dm": "Dominica",
    ".do": "Dominican Republic",
    ".dz": "Algeria",
    ".ec": "Ecuador",
    ".ee": "Estonia",
    ".eg": "Egypt",
    ".er": "Eritrea",
    ".es": "Spain",
    ".et": "Ethiopia",
    ".eu": "European Union",
    ".fi": "Finland",
    ".fj": "Fiji",
    ".fk": "Falkland Islands",
    ".fm": "Federated States of Micronesia",
    ".fo": "Faroe Islands",
    ".fr": "France",
    ".ga": "Gabon",
    ".gb": "United Kingdom",
    ".gd": "Grenada",
    ".ge": "Georgia",
    ".gf": "French Guiana",
    ".gg": "Guernsey",
    ".gh": "Ghana",
    ".gi": "Gibraltar",
    ".gl": "Greenland",
    ".gm": "Gambia",
    ".gn": "Guinea",
    ".gp": "Guadeloupe",
    ".gq": "Equatorial Guinea",
    ".gr": "Greece",
    ".gs": "South Georgia and the South Sandwich Islands",
    ".gt": "Guatemala",
    ".gu": "Guam",
    ".gw": "Guinea-Bissau",
    ".gy": "Guyana",
    ".hk": "Hong Kong",
    ".hm": "Heard Island and McDonald Islands",
    ".hn": "Honduras",
    ".hr": "Croatia",
    ".ht": "Haiti",
    ".hu": "Hungary",
    ".id": "Indonesia",
    ".ie": "Ireland",
    ".il": "Israel",
    ".im": "Isle of Man",
    ".in": "India",
    ".io": "British Indian Ocean Territory",
    ".iq": "Iraq",
    ".ir": "Iran",
    ".is": "Iceland",
    ".it": "Italy",
    ".je": "Jersey",
    ".jm": "Jamaica",
    ".jo": "Jordan",
    ".jp": "Japan",
    ".ke": "Kenya",
    ".kg": "Kyrgyzstan",
    ".kh": "Cambodia",
    ".ki": "Kiribati",
    ".km": "Comoros",
    ".kn": "Saint Kitts and Nevis",
    ".kp": "Democratic People's Republic of Korea (North Korea)",
    ".kr": "Republic of Korea (South Korea)",
    ".kw": "Kuwait",
    ".ky": "Cayman Islands",
    ".kz": "Kazakhstan",
    ".la": "Laos",
    ".lb": "Lebanon",
    ".lc": "Saint Lucia",
    ".li": "Liechtenstein",
    ".lk": "Sri Lanka",
    ".lr": "Liberia",
    ".ls": "Lesotho",
    ".lt": "Lithuania",
    ".lu": "Luxembourg",
    ".lv": "Latvia",
    ".ly": "Libya",
    ".ma": "Morocco",
    ".mc": "Monaco",
    ".md": "Moldova",
    ".me": "Montenegro",
    ".mf": "Saint Martin (French part)",
    ".mg": "Madagascar",
    ".mh": "Marshall Islands",
    ".mk": "North Macedonia",
    ".ml": "Mali",
    ".mm": "Myanmar",
    ".mn": "Mongolia",
    ".mo": "Macao",
    ".mp": "Northern Mariana Islands",
    ".mq": "Martinique",
    ".mr": "Mauritania",
    ".ms": "Montserrat",
    ".mt": "Malta",
    ".mu": "Mauritius",
    ".mv": "Maldives",
    ".mw": "Malawi",
    ".mx": "Mexico",
    ".my": "Malaysia",
    ".mz": "Mozambique",
    ".na": "Namibia",
    ".nc": "New Caledonia",
    ".ne": "Niger",
    ".nf": "Norfolk Island",
    ".ng": "Nigeria",
    ".ni": "Nicaragua",
    ".nl": "Netherlands",
    ".no": "Norway",
    ".np": "Nepal",
    ".nr": "Nauru",
    ".nu": "Niue",
    ".nz": "New Zealand",
    ".om": "Oman",
    ".pa": "Panama",
    ".pe": "Peru",
    ".pf": "French Polynesia",
    ".pg": "Papua New Guinea",
    ".ph": "Philippines",
    ".pk": "Pakistan",
    ".pl": "Poland",
    ".pm": "Saint Pierre and Miquelon",
    ".pn": "Pitcairn",
    ".pr": "Puerto Rico",
    ".ps": "Palestinian Territory",
    ".pt": "Portugal",
    ".pw": "Palau",
    ".py": "Paraguay",
    ".qa": "Qatar",
    ".re": "Réunion",
    ".ro": "Romania",
    ".rs": "Serbia",
    ".ru": "Russia",
    ".rw": "Rwanda",
    ".sa": "Saudi Arabia",
    ".sb": "Solomon Islands",
    ".sc": "Seychelles",
    ".sd": "Sudan",
    ".se": "Sweden",
    ".sg": "Singapore",
    ".sh": "Saint Helena",
    ".si": "Slovenia",
    ".sj": "Svalbard and Jan Mayen",
    ".sk": "Slovakia",
    ".sl": "Sierra Leone",
    ".sm": "San Marino",
    ".sn": "Senegal",
    ".so": "Somalia",
    ".sr": "Suriname",
    ".ss": "South Sudan",
    ".st": "São Tomé and Príncipe",
    ".sv": "El Salvador",
    ".sx": "Sint Maarten (Dutch part)",
    ".sy": "Syria",
    ".sz": "Eswatini",
    ".tc": "Turks and Caicos Islands",
    ".td": "Chad",
    ".tf": "French Southern Territories",
    ".tg": "Togo",
    ".th": "Thailand",
    ".tj": "Tajikistan",
    ".tk": "Tokelau",
    ".tl": "Timor-Leste",
    ".tm": "Turkmenistan",
    ".tn": "Tunisia",
    ".to": "Tonga",
    ".tr": "Turkey",
    ".tt": "Trinidad and Tobago",
    ".tv": "Tuvalu",
    ".tw": "Taiwan",
    ".tz": "Tanzania",
    ".ua": "Ukraine",
    ".ug": "Uganda",
    ".uk": "United Kingdom",
    ".us": "United States",
    ".uy": "Uruguay",
    ".uz": "Uzbekistan",
    ".va": "Vatican City",
    ".vc": "Saint Vincent and the Grenadines",
    ".ve": "Venezuela",
    ".vg": "British Virgin Islands",
    ".vi": "U.S. Virgin Islands",
    ".vn": "Vietnam",
    ".vu": "Vanuatu",
    ".wf": "Wallis and Futuna",
    ".ws": "Samoa",
    ".ye": "Yemen",
    ".yt": "Mayotte",
    ".za": "South Africa",
    ".zm": "Zambia",
    ".zw": "Zimbabwe"
    }
    
    for ccTLD in ccTLD_to_region:
        if primary_domain.endswith(ccTLD):
            return ccTLD_to_region[ccTLD]
    
    return "Global"
def extract_root_domain(url):
    extracted = tldextract.extract(url)
    root_domain = extracted.domain
    return root_domain
def hash_encode(category):
    hash_object = hashlib.md5(category.encode())
    return int(hash_object.hexdigest(), 16) % (10 ** 8)


# In[99]:


def model_predict(url):
    class_mapping = {
        0: 'benign',
        1: 'defacement',
        2: 'phishing',
        3: 'malware'
    }
    numerical_values = get_numerical_values(url)
    prediction_int = pipeline.predict(np.array(list(numerical_values.values())).reshape(1, -1))[0]
    prediction_label = class_mapping.get(prediction_int, 'Unknown')
    return prediction_int, prediction_label


# Preprocessing functions
def preprocess_xss(sentence):
    sentence_ascii = [ord(char) for char in sentence if ord(char) <= 128]
    zer = np.zeros((10000))
    for i, val in enumerate(sentence_ascii[:10000]):
        zer[i] = val
    image = cv2.resize(zer.reshape((100, 100)), (100, 100), interpolation=cv2.INTER_CUBIC)
    return image.reshape(1, 100, 100, 1) / 128  # Normalize

def preprocess_url(url):
    # Placeholder for actual preprocessing
    # Replace `get_numerical_values` with the actual implementation
    numerical_values = get_numerical_values(url)
    return np.array(list(numerical_values.values())).reshape(1, -1)

from scipy.sparse import hstack, csr_matrix

def preprocess_sql(query):
    def process(x, pattern):
        return len(re.findall(pattern, x))

    def combined_keywords(x):
        patterns = [r'null', r'chr', r'char']
        return sum(len(re.findall(p, x)) for p in patterns)

    def genuine(x):
        genuine_keys = ['select', 'top', 'order', 'fetch', 'join', 'avg', 'count', 'sum', 'rows']
        return sum(x.split().count(key) for key in genuine_keys)

    preprocessed_query = [
        process(query, "'"),
        process(query, '"'),
        process(query, r"[!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]"),
        process(query, r'(--)'),
        process(query, r'(\/\*)'),
        process(query, r'\s+'),
        process(query, r"%"),
        process(query, r'\snot\s|\sand\s|\sor\s|\sxor\s|&&|\|\||!'),
        process(query, r"'\+|-|[^\/]\*|\/[^\*]'"),
        process(query, "null"),
        process(query, r'0[xX][0-9a-fA-F]+\s'),
        process(query, r'[a-zA-Z]'),
        process(query, r'[0-9]'),
        combined_keywords(query),
        genuine(query)
    ]

    unigram_bow = train_bow.transform([query])  # shape: (1, N)
    preprocessed_query_sparse = csr_matrix([preprocessed_query])  # shape: (1, 15)

    return hstack((unigram_bow, preprocessed_query_sparse))


# Route definitions
# Import necessary libraries
from flask import Flask, render_template, request, jsonify



# Route definitions
@app.route('/')
def home():
    return render_template('home.html')

@app.route('/xss', methods=['GET', 'POST'])
def xss():
    if request.method == 'POST':
        if request.content_type != 'application/json':
            return jsonify({"error": "Content-Type must be application/json"}), 400

        input_data = request.get_json()
        input_text = input_data.get('input_text', '').lower()

        if not input_text:
            return jsonify({"error": "Invalid input"}), 400

        # Preprocess and predict
        processed = preprocess_xss(input_text)  # Assume this returns the right format
        prediction = xss_model.predict(processed)[0][0]
        result = "Attack Detected" if prediction > 0.5 else "Benign"

        return jsonify({
            "input": input_text,
            "type": "xss",
            "result": result
        })

    # Handle GET request (serves the HTML page)
    return render_template('xss.html')

@app.route('/sql', methods=['GET', 'POST'])
def sql():
    if request.method == 'POST':
        print("xss input recived")
        input_data = request.get_json()
        input_text = input_data.get('input_text', '').lower()

        if not input_text:
            return jsonify({"error": "Invalid input"}), 400

        # Placeholder: Preprocess and predict for SQL Injection
        processed = preprocess_sql(input_text)
        prediction = sql_injection_model.predict(processed)[0]
        result = "SQL Injection Detected" if prediction == 1 else "Benign Query"

        return jsonify({
            "input": input_text,
            "type": "sql",
            "result": result
        })

    return render_template('sql.html')

@app.route('/url', methods=['GET', 'POST'])
def url():
    if request.method == 'POST':
        print("xss input recived")
        input_data = request.get_json()
        input_text = input_data.get('input_text', '').lower()

        if not input_text:
            return jsonify({"error": "Invalid input"}), 400

        # Placeholder: Preprocess and predict for Malicious URL
        processed = preprocess_url(input_text)
        prediction = url_model.predict(processed)[0]
        result = "Malicious" if prediction else "Safe"

        return jsonify({
            "input": input_text,
            "type": "url",
            "result": result
        })

    return render_template('url.html')
def check_sql_injection(url):
    test_payload = "' OR '1'='1"
    try:
        response = requests.get(f"{url}?id={test_payload}", timeout=5)
        if "syntax error" in response.text.lower() or "database" in response.text.lower():
            return True
    except requests.exceptions.RequestException:
        pass
    return False

# Helper Function to Check for Missing Security Headers
def check_security_headers(response):
    vulnerabilities = []
    headers = response.headers
    
    if "Content-Security-Policy" not in headers:
        vulnerabilities.append("Missing Content-Security-Policy header.")
    if "X-Content-Type-Options" not in headers:
        vulnerabilities.append("Missing X-Content-Type-Options header.")
    if "Strict-Transport-Security" not in headers:
        vulnerabilities.append("Missing Strict-Transport-Security header.")
    if "X-Frame-Options" not in headers:
        vulnerabilities.append("Missing X-Frame-Options header.")
    
    return vulnerabilities

# Helper Function to Check for XSS Vulnerabilities
def check_xss(url):
    vulnerabilities = []
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all("form")
        for form in forms:
            action = form.get("action") or ""
            inputs = form.find_all("input")
            for input_tag in inputs:
                name = input_tag.get("name")
                if name:
                    xss_payload = "<script>alert('XSS')</script>"
                    target_url = url + action
                    xss_test = requests.post(target_url, data={name: xss_payload}, timeout=5)
                    if xss_payload in xss_test.text:
                        vulnerabilities.append(f"Potential XSS vulnerability in form: {action}")
    except requests.exceptions.RequestException:
        pass
    
    return vulnerabilities

# Home Page with Input Form
@app.route('/index2')
def index2():
    return render_template('index2.html')

# Scan URL and Display Results
@app.route('/scan', methods=['POST'])
def scan_website():
    input_url = request.form.get("url")
    
    # Validate URL input
    if not input_url:
        return render_template("index.html", error="Please provide a URL to scan.")
    
    # Add protocol if missing
    if not input_url.startswith(('http://', 'https://')):
        input_url = f"http://{input_url}"
    
    # Initialize results dictionary
    results = {"url": input_url, "vulnerabilities": [], "headers": {}, "xss_forms": []}
    
    try:
        # Check for Missing Security Headers
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(input_url, headers=headers, timeout=10)
        results["headers"] = dict(response.headers)  # Store headers for display
        
        # Check SQL Injection
        if check_sql_injection(input_url):
            results["vulnerabilities"].append("Potential SQL Injection detected.")
        
        # Check Missing Headers
        header_vulnerabilities = check_security_headers(response)
        results["vulnerabilities"].extend(header_vulnerabilities)
        
        # Check XSS Vulnerabilities
        xss_vulnerabilities = check_xss(input_url)
        results["xss_forms"].extend(xss_vulnerabilities)
        
    except requests.exceptions.Timeout:
        return render_template("index2.html", error="Connection timed out. Please try again.")
    except requests.exceptions.ConnectionError:
        return render_template("index2.html", error="Failed to connect to the URL. Please check the address.")
    except Exception as e:
        return render_template("index2.html", error=f"An unexpected error occurred: {str(e)}")
    
    return render_template("results.html", results=results)


if __name__ == '__main__':
    app.run(debug=True)
